
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Incremental push/pull &#8212; Breezy 3.0.2dev documentation</title>
    <link rel="stylesheet" href="../../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>

    <link rel="search" title="Search" href="../../search.html" />
<link rel="stylesheet" href="_static/brz-doc.css" type="text/css" />
 
  </head><body>
    <div class="header-wrapper" role="banner">
      <div class="header">
        <div class="headertitle"><a
          href="../../index.html">Developer Document Catalog (3.0.2dev)</a></div>
        <div class="rel" role="navigation" aria-label="related navigation">
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="incremental-push-pull">
<h1>Incremental push/pull<a class="headerlink" href="#incremental-push-pull" title="Permalink to this headline">¶</a></h1>
<p>This use case covers pulling in or pushing out some number of revisions which
is typically a small fraction of the number already present in the target
repository. Pushing and pulling are defined as branch level operations for ease
of interaction with VCS systems that have no repository abstraction (such as
bzr-svn or GNU Arch) but within bzrlib’s core they are currently the
responsibility of the Repository object.</p>
<div class="section" id="functional-requirements">
<h2>Functional Requirements<a class="headerlink" href="#functional-requirements" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>A push or pull operation must:</dt><dd><ul class="simple">
<li><p>Copy all the data to reconstruct the selected revisions in the target
branch. This is the goal of push and pull after all.</p></li>
<li><p>Reject corrupt data. As brz has no innate mechanism for discarding corrupted
data, corrupted data should not be incorporated accidentally.</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="factors-which-should-add-work-for-push-pull">
<h2>Factors which should add work for push/pull<a class="headerlink" href="#factors-which-should-add-work-for-push-pull" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>Baseline overhead: The time to connect to both branches.</p></li>
<li><p>Actual new data in the revisions being pulled (drives the amount of data to
move around, includes the commit messages etc)</p></li>
<li><p>Number of revisions in the two repositories (scaling affects the
determination of what revisions to move around).</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="push-pull-overview">
<h2>Push/pull overview<a class="headerlink" href="#push-pull-overview" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>New data is identified in the source repository.</p></li>
<li><p>That data is read from the source repository.</p></li>
<li><p>The same data is verified and written to the target repository in such a
manner that it’s not visible to readers until it’s ready for use.</p></li>
</ol>
<div class="section" id="new-data-identification">
<h3>New data identification<a class="headerlink" href="#new-data-identification" title="Permalink to this headline">¶</a></h3>
<p>We have a single top level data object: revisions. Everything else is
subordinate to revisions, so determining the revisions to propagate should be
all thats needed. This depends on revisions with partial data - such as those
with no signature - being flagged in some efficient manner.</p>
<p>We could do this in two manners: determine revisions to sync and signatures to sync in two passes, or change the ‘value’ of a revision implicitly when the signature is different. E.g. by using merkle hash trees with the signature data a separate component the signatures will naturally be identified to sync.</p>
<p>We want to only exchange data proportional to the number of new revisions and
signatures in the system though. One way to achieve this for revisions is to
walk the graph out from the desired tips until the surface area intersection is
found. For signatures a set difference seems to be needed as there is no DAG of signatures: the presence of one has no implications on the presence of another, so a full pass over the set of signatures would be required to confirm no new signatures are needed (let alone replaced signatures).</p>
<p>IFF we can determine ‘new revisions’ and ‘new signatures’ without full graph access then we can scale acceptable for push and pull.</p>
<p>Ghosts are revisions which are not present in a particular repository. Filling ghosts refers to removing ghosts in the target repository when the ghost is present in the source repository. Filling ghosts can be either an explicit or implicit action. The common case is no ghosts.</p>
<div class="section" id="set-synchronisation-approaches">
<h4>Set synchronisation approaches<a class="headerlink" href="#set-synchronisation-approaches" title="Permalink to this headline">¶</a></h4>
<p>A set synchronisation approach is one which synchronises two sets without
regard for innate structure. This can be very efficient but requires adding a
new node to be processed with every commit. Caching of the results of the
various set based syncs I’ve seen is possible but because the data structures
look different depending on the tip revision being synced up to the cache needs
to be very complex. I recommend not using such an approach for the common case
pull because of the failure to scale. We can use such an approach for
synchronisation of new signatures and ghosts, which should be an explicit
option in both cases.</p>
</div>
<div class="section" id="dag-synchronisation-approaches">
<h4>DAG synchronisation approaches<a class="headerlink" href="#dag-synchronisation-approaches" title="Permalink to this headline">¶</a></h4>
<p>A DAG based approach to synchronistion is one that uses the DAG structure to
determine the difference in present nodes. It can as a result operate from the
tip of the DAG backwards. A dag based approach should allow incremental access
to data and not require a full-graph scan for incremental operations.</p>
</div>
<div class="section" id="file-level-scaling">
<h4>File level scaling<a class="headerlink" href="#file-level-scaling" title="Permalink to this headline">¶</a></h4>
<p>We should read roughly as much of the revision level graph as is needed from
each repository to determine the node difference.  If requested we should
perform a detailed scan to pick up ghost revisions and revisions which have had
signatures added. This should not be the default as it requires full history
access in both cases.</p>
<p>Expected file IO and access pattern:</p>
<blockquote>
<div><ul>
<li><p>Common case: repo with many branches of one project, to the same.</p>
<ol class="arabic simple">
<li><p>Source and Target branch tips read.</p></li>
<li><p>Find the tip of each branch in their repo (will require reading some of
the revision graph but is typically near the end of the graph).</p></li>
<li><p>Read and parse increasing amounts of the revision graph until one is
found to be a subset of the other, or a complete list of revisions to be
transmitted is created.</p></li>
</ol>
</li>
<li><p>Uncommon cases:</p>
<ol class="arabic simple">
<li><p>Repositories with many projects or branches which are very old may
require reading a lot of unrelated graph data.</p></li>
</ol>
<ol class="arabic simple">
<li><p>Initial push/pull scenarios should not require reading an entire graph.</p></li>
</ol>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="api-scaling">
<h4>API scaling<a class="headerlink" href="#api-scaling" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ol class="arabic simple">
<li><p>Get branch tips.</p></li>
<li><p>Determine one sided graph difference. To avoid obtaining a full graph over
the wire this needs to be done without reference to the full graph, and
with some logarthmic scaling algorithm. There are several already available
for this.</p></li>
</ol>
</div></blockquote>
<p>With ghost and new-signature detection:</p>
<blockquote>
<div><ul class="simple">
<li><p>File IO access pattern will read the entire graph on the ‘target’ side - if
no ghosts are present then stop, otherwise seek the new revisions on the
source side with the regular algorithm and also explicitly search for the
ghost points from the target; plus a set difference search is needed on
signatures.</p></li>
<li><p>Semantic level can probably be tuned, but as it’s also complex I suggest
deferring analysis for optimal behaviour of this use case.</p></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="data-reading">
<h3>Data reading<a class="headerlink" href="#data-reading" title="Permalink to this headline">¶</a></h3>
<p>When transferring information about a revision the graph of data for the
revision is walked: revision -&gt; inventory, revision -&gt; matching signature,
inventory -&gt; file ids:revision pairs.</p>
<div class="section" id="id1">
<h4>File level scaling<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>As we’re reading already committed data, as long as nothing is mutating data on
disk reading should be race free. We will:</p>
<blockquote>
<div><ul class="simple">
<li><p>read each revision object</p></li>
<li><p>read the matching inventory delta</p></li>
<li><p>attempt to read a signature object</p></li>
<li><p>parse the inventory delta</p></li>
<li><p>read the fileid:revisionid compressed chunk for each line in the inventory
delta</p></li>
</ul>
</div></blockquote>
<p>Theres no point validating that the data read is valid, as transmission through to the client writing the data might invalidate it; we need to validate before we write.</p>
</div>
<div class="section" id="id2">
<h4>API scaling<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>Given that we have established the revisions needed, a single API call should
suffice to obtain all data; the API should present the data in such an order
that it can be validated as it arrives and thus not require large scale
buffering on disk. Specifically each item of data should be validatable (e.g.
for some file data we want the fileid:revisionid:validationhash + content).</p>
</div>
</div>
<div class="section" id="data-verification-and-writing">
<h3>Data Verification and writing<a class="headerlink" href="#data-verification-and-writing" title="Permalink to this headline">¶</a></h3>
<p>New data written to a repository should be completed intact when it is made
visible. This suggests that either all the data for a revision must be made
atomically visible (e.g. by renaming a single file) or the leaf nodes of the
reference graph must become visible first.</p>
<p>Data is referred to via the following graph:
revision -&gt; revision
revision -&gt; signature
revision -&gt; inventory
inventory -&gt; fileid:revisionid
fileid:revisionid -&gt; fileid:revisionid</p>
<p>Data is verifiable via a different ordering:
signature -&gt; revision -&gt; inventory -&gt; fileid:revisionid texts.</p>
<p>We dont gpg verify each revision today; this analysis only speaks to hash
verification of contents.</p>
<p>To validate a revision we need to validate the data it refers to. But to
validate the contents of a revision we need the new texts in the inventory for
the revision - to check a fileid:revisionid we need to know the expected sha1
of the full text and thus also need to read the delta chain to construct the
text as we accept it to determine if it’s valid. Providing separate validators
for the chosen representation would address this.
e.g: For an inventory entry FILEID:REVISIONID we store the validator of the
full text :SHA1:. If we also stored the validator of the chosen disk
representation (:DELTASHA1:) we could validate the transmitted representation
without expanding the delta in the common case. If that failed we could expand
the delta chain and try against the full text validator, and finally fail. As
different delta generators might generate different deltas, :DELTASHA1: should
not become part of the revision validator, only the inventory disk encoding. In
a related manner a transmission format that allowed cheap validation of content
without applying locally stored deltas would be advantageous because no local
reads would be incurred to validate new content. For instance, always sending a
full text for any file, possibly with a delta-chain when transmitting multiple
revisionids of the file, would allow this. (git pack-files have this property).</p>
<div class="section" id="overview-summary">
<h4>Overview summary<a class="headerlink" href="#overview-summary" title="Permalink to this headline">¶</a></h4>
<p>A single-file local format would allow safe atomic addition of data while
allowing optimisal transmission order of data. Failing this the validation of
data should be tuned to not require reading local texts during data addition
even in the presence of delta chains. We should have transmission-validators
separate from content validators that allow validation of the delta-transmitted
form of objects.</p>
</div>
<div class="section" id="id3">
<h4>File level scaling<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Every new file text requires transmission and local serialisation.</p></li>
<li><p>Every commit requires transmission and storage of a revision, signature and inventory.</p></li>
</ul>
<p>Thus 4000 commits to a 50000 path tree of 10 files on averages requires (with
knits) between 26 writes (2*(3+10)) and 80006 (2*(4000*10 + 3)) writes. In all
cases there are 4000 * 13 distinct objects to record.</p>
<p>Grouping data by fileid, content and metadata, gives the figures above.
Data grouping:</p>
<ul class="simple">
<li><p>File per full identifier (fileid:revisionid:meta|content): 104000</p></li>
<li><p>Delta-chain per object: object id count * constant overhead per object id
(26 -&gt; 80006)</p></li>
<li><p>Collation/pack file: 1</p></li>
</ul>
<dl class="simple">
<dt>Performance for these depends heavily on implementation:</dt><dd><ul class="simple">
<li><p>Using full ids we could name by validator or by id, giving best performance
that depends on either receiving data in validator order or in id order.</p></li>
<li><p>using delta-chain per object we get least seek overhead and syscall overhead
if we recieve in topological order within the object id, and object ids in
lexical order.</p></li>
<li><p>Using a collation/pack file we can stream it into place and validate as we go,
giving near ideal performance.</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="id4">
<h4>API scaling<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>The api for writing new data recieved over the network will need to be geared
to the transmission and local storage method. What we need is for the
transmission method to reasonably closely match the desired write ordering
locally. This suggests that once we decide on the best local storage means we
should design the api.</p>
<p>take N commits from A to B, if B is local then merge changes into the tree.
copy ebough data to recreate snapshots
avoid ending up wth corrupt/bad data</p>
</div>
</div>
</div>
<div class="section" id="notes-from-london">
<h2>Notes from London<a class="headerlink" href="#notes-from-london" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ol class="arabic simple">
<li><p>setup</p></li>
</ol>
<blockquote>
<div><p>look at graph of revisions for ~N comits to deretmine eligibility for
if preserve mainline is on, check LH only</p>
<blockquote>
<div><dl class="simple">
<dt>identify objects to send that are not on the client repo</dt><dd><ul class="simple">
<li><p>revision - may be proportional to the graph</p></li>
<li><p>inventory - proportional to work</p></li>
<li><p>texts     - proportional to work</p></li>
<li><p>signatures - ???</p></li>
</ul>
</dd>
</dl>
</div></blockquote>
</div></blockquote>
<ol class="arabic simple">
<li><p>data transmission</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>send data proportional to the new information</p></li>
<li><p>validate the data:</p></li>
</ul>
<blockquote>
<div><ol class="arabic simple">
<li><p>validate the sha1 of the full text of each transmitted text.</p></li>
<li><p>validate the sha1:name mapping in each newly referenced inventory item.</p></li>
<li><p>validate the sha1 of the XML of each inventory against the revision.
<strong>this is proportional to tree size and must be fixed</strong></p></li>
</ol>
</div></blockquote>
</div></blockquote>
<ol class="arabic simple">
<li><p>write the data to the local repo.
The API should output the file texts needed by the merge as by product of the transmission</p></li>
<li><p>tree application</p></li>
</ol>
</div></blockquote>
<p>Combine the output from the transmission step with additional ‘new work data’ for anything already in the local repository that is new in this tree.
should write new files and stat existing files proportional to the count of the new work and the size of the full texts.</p>
</div>
</div>


          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          <h3>Table of Contents</h3>
          <ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribution-quickstart.html">Contributing to Breezy</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../profiling.html">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bug-handling.html">Tracking Bugs in Breezy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../HACKING.html">Breezy Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../testing.html">Breezy Testing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../code-review.html">Reviewing proposed changes to Breezy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../code-style.html">Breezy Code Style Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../documenting-changes.html">Documenting Changes</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../configuration.html">Configuring Breezy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../configuration.html#breezy-conf">breezy.conf</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../configuration.html#location-conf">location.conf</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../configuration.html#branch-conf">branch.conf</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fetch.html">Fetching data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../transports.html">Developer guide to breezy transports</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ui.html">Interacting with the user</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../releasing.html">Releasing Breezy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ppa.html">Managing the Breezy PPA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ec2.html">Breezy Windows EC2 Server</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Breezy Architectural Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../integration.html">Integrating with Breezy</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../principles.html">Breezy Design Principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../specifications.html">Specifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../implementation-notes.html">Implementation notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../miscellaneous-notes.html">Miscellaneous notes</a></li>
</ul>

          <div role="search">
            <h3 style="margin-top: 1.5em;">Search</h3>
            <form class="search" action="../../search.html" method="get">
                <input type="text" name="q" />
                <input type="submit" value="Go" />
            </form>
          </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <div role="navigation" aria-label="related navigaton">
          </div>
          <div role="note" aria-label="source link">
              <br/>
              <a href="../../_sources/plans/performance/incremental-push-pull.txt"
                rel="nofollow">Show Source</a>
          </div>
        </div>

        <div class="right">
          
    <div class="footer" role="contentinfo">
        &#169; Copyright 2009-2011 Canonical Ltd, 2017-2018 Breezy Developers.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.2.1.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>