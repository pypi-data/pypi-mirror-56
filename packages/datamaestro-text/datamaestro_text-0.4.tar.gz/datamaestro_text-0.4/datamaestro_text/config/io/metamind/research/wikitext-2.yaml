# Wiki-103 dataset

# See http://experimaestro.github.io/datamaestro/configuration/ for configuration documentation

name: Wiki-2
abstract: true
web: https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/

links:
  paper: https://arxiv.org/abs/1609.07843


tags: [ "text" ]
tasks: [ "language modeling" ]


description: |
  The WikiText language modeling dataset is a collection of over 100 million
  tokens extracted from the set of verified Good and Featured articles on
  Wikipedia. The dataset is available under the Creative Commons
  Attribution-ShareAlike License.

  Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over
  2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset
  also features a far larger vocabulary and retains the original case, punctuation
  and numbers - all of which are removed in PTB. As it is composed of full
  articles, the dataset is well suited for models that can take advantage of long
  term dependencies.

---
id: word
download: !@/archive:Zip
  url: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip
  
---
id: raw
download: !@/archive:Zip
  url: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip
