import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from itertools import ifilterfalse
except ImportError:
    from itertools import filterfalse


def _mean(l, ignore_nan=False, empty=0):
    """
    nanmean compatible with generators.
    """
    l = iter(l)
    if ignore_nan:
        l = ifilterfalse(np.isnan, l)
    try:
        n = 1
        acc = next(l)
    except StopIteration:
        if empty == "raise":
            raise ValueError("Empty mean")
        return empty
    for n, v in enumerate(l, 2):
        acc += v
    if n == 1:
        return acc
    return acc / n


def _lovasz_grad(gt_sorted):
    """
    Computes gradient of the Lovasz extension w.r.t sorted errors
    See Alg. 1 in paper
    """
    p = len(gt_sorted)
    gts = gt_sorted.sum()
    intersection = gts - gt_sorted.float().cumsum(0)
    union = gts + (1 - gt_sorted).float().cumsum(0)
    jaccard = 1.0 - intersection / union
    if p > 1:  # cover 1-pixel case
        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]
    return jaccard


def _lovasz_hinge_flat(logits, labels):
    """
    Binary Lovasz hinge loss
      logits: [P] Variable, logits at each prediction (between -\infty and +\infty)
      labels: [P] Tensor, binary ground truth labels (0 or 1)
      ignore: label to ignore
    """
    if len(labels) == 0:
        # only void pixels, the gradients should be 0
        return logits.sum() * 0.0
    signs = 2.0 * labels.float() - 1.0
    errors = 1.0 - logits * signs.requires_grad_()
    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)
    perm = perm.data
    gt_sorted = labels[perm]
    grad = _lovasz_grad(gt_sorted)
    loss = torch.dot(F.elu(errors_sorted) + 1, grad.requires_grad_())
    return loss


def _flatten_binary_scores(scores, labels, ignore=None):
    """
    Flattens predictions in the batch (binary case)
    Remove labels equal to 'ignore'
    """
    scores = scores.view(-1)
    labels = labels.view(-1)
    if ignore is None:
        return scores, labels
    valid = labels != ignore
    vscores = scores[valid]
    vlabels = labels[valid]
    return vscores, vlabels


def lovasz_hinge(logits, labels, per_image=True):
    """The binary Lovasz Hinge loss for semantic segmentation.

    Implementation adapted from https://github.com/bermanmaxim/LovaszSoftmax
    
    Args:
        logits (torch.tensor): Logits at each pixel (between -\infty and +\infty).
        labels (torch.tensor): Binary ground truth masks (0 or 1).
        per_image (bool, optional): Compute the loss per image instead of per batch.
                                    Defaults to True.

    Shape:
        - Input:
            - logits: (batch, height, width)
            - labels: (batch, height, width)
        - Output: (batch)

    Returns:
        torch.tensor: The lovasz hinge loss
    """
    if per_image:
        loss = _mean(
            _lovasz_hinge_flat(
                *_flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), None)
            )
            for log, lab in zip(logits, labels)
        )
    else:
        loss = _lovasz_hinge_flat(*_flatten_binary_scores(logits, labels, None))
    return loss

class DiceLoss(nn.Module):
    """The dice loss for semantic segmentation

    Implementation adapted from https://www.kaggle.com/soulmachine/siim-deeplabv3

    Shape:
        - Input:
            - logits: (batch, *)
            - targets: (batch, *) _same shape as logits_
        - Output: (1)

    Returns:
        torch.tensor: The dice loss
        
    """
    def __init__(self):
        super(DiceLoss, self).__init__()

    def forward(self, logits, targets):
        N = targets.size(0)
        preds = torch.sigmoid(logits)

        EPSILON = 1

        preds_flat = preds.view(N, -1)
        targets_flat = targets.view(N, -1)

        intersection = (preds_flat * targets_flat).sum()
        union = (preds_flat + targets_flat).sum()

        loss = (2.0 * intersection + EPSILON) / (union + EPSILON)
        loss = 1 - loss / N

        return loss