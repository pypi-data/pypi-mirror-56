{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My white 🐳 is literate markdown programming in the notebook.\n",
    "\n",
    "I have made so many attempts at a literate interface to the notebook.  Each of these experiments revealed important features of modern [literate ~~programming~~ computing](http://blog.fperez.org/2013/04/literate-computing-and-computational.html).\n",
    "\n",
    "I've drawn the concept in packages & single notebooks.  In retrospect, they contain too many opinions to be extensible.  \n",
    "\n",
    "Here is another go at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import re, textwrap, tokenize, io, itertools, IPython; from toolz.curried import *\n",
    "    __all__ = 'parse',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `mistune` to `parse` block level markdown objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Lexer(__import__('mistune').BlockLexer):\n",
    "        def parse(self, text, rules=None):\n",
    "            text = ''.join(x if x.strip() else \"\\n\" for x in text.splitlines(True))\n",
    "            rules = rules or self.default_rules\n",
    "            def manipulate(text):\n",
    "                for key in rules:\n",
    "                    m = getattr(self.rules, key).match(text)\n",
    "                    if m: getattr(self, 'parse_%s' % key)(m); return m\n",
    "                return False  \n",
    "            while text:\n",
    "                m = manipulate(text)\n",
    "                if m: text = text[len(m.group(0)):]\n",
    "                if not m and text: raise RuntimeError('Infinite loop at: %s' % text)\n",
    "                if self.tokens: self.tokens[-1]['match'] = m\n",
    "            return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def quote(str, prior=\"\", tick='\"\"\"'):\n",
    "        \"\"\"wrap a block of text in quotes. \"\"\"\n",
    "        if not str.strip(): return str\n",
    "        indent, outdent = len(str)-len(str.lstrip()), len(str.rstrip())\n",
    "        if tick in str or str.endswith(tick[0]): tick = '\"\"\"'\n",
    "        return str[:indent] + prior + tick + str[indent:outdent] + tick + \";\" + str[outdent:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse`  markdown code into valid python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class Parser:\n",
    "        def parse_code(self, token, *, indent=-1):\n",
    "            code = token['match'].group().rstrip(); stripped = code.strip()\n",
    "            if stripped.startswith(('>>>',)) or (not stripped and 'match' in token): return # don't do anything for blank code.\n",
    "            if code.startswith(('```',)): \n",
    "                code = ''.join(['\\n'] + code.rstrip('`').splitlines(True)[1:])\n",
    "                if textwrap.dedent(code) == code: code = textwrap.indent(code, ' '*max(indent, 4))\n",
    "            return code\n",
    "        \n",
    "        def indent_text(self, body, text, code=\"\", *, indent=-1, block_level=0):\n",
    "            try: tokenized = list(tokenize.tokenize(io.BytesIO(textwrap.dedent(body).encode('utf-8')).readline)) if body else []\n",
    "            except tokenize.TokenError as exception:\n",
    "                if exception.args[0] == 'EOF in multi-line string': text = textwrap.indent(text, ' '*_indent)\n",
    "                else: text = (text.strip() and quote or identity)(text, ' '*_indent)\n",
    "            else: \n",
    "                while tokenized and not tokenized[-1].string: tokenized.pop()\n",
    "                this_indent, line = indent, tokenized[-1].line if tokenized else \"\"\n",
    "                this_indent += len(line) - len(line.lstrip())\n",
    "                if body.rstrip().endswith(':'): \n",
    "                    for last in code.splitlines() or ['']:\n",
    "                        if last.strip(): break\n",
    "                    this_indent += max(len(last)-len(last.lstrip())-this_indent + block_level*2, 4)\n",
    "                text = quote(textwrap.indent(text, ' '*this_indent)) \n",
    "            return body + text + code\n",
    "        \n",
    "        def parse(self, object, *, formatted=\"\", indent=-1, block_level=0):\n",
    "            tokens, attrs = Lexer()(object), set(dir(self))\n",
    "            while tokens:\n",
    "                token = tokens.pop(0)\n",
    "                if token['type'] == 'list_start': block_level += 1\n",
    "                if token['type'] == 'list_end': block_level -= 1\n",
    "                if F\"parse_{token['type']}\" in attrs: \n",
    "                    code = getattr(self, F\"parse_{token['type']}\")(token, indent=indent)\n",
    "                    if code is None: continue\n",
    "                    if indent < 0: indent = max(len(code) - len(code.lstrip()), 4)\n",
    "                    text, object = re.split(r'\\s*'.join(re.escape(token['match'].group().rstrip()).splitlines(True)), object)\n",
    "                    formatted = self.indent_text(formatted, text, code, indent=indent, block_level=block_level)\n",
    "            return self.indent_text(formatted, object, indent=indent)\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another notebook defines how the output should be displayed.\n",
    "\n",
    "Create `IPython` extensions that can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "    parser = Parser()\n",
    "    def cleanup_transform(x): \n",
    "        global parser\n",
    "        return textwrap.dedent(parser.parse(''.join(x))).splitlines(True)\n",
    "\n",
    "    def unload_ipython_extension(shell):\n",
    "        globals()['_transforms'] = globals().get('_transforms', shell.input_transformer_manager.cleanup_transforms)\n",
    "        global _transforms\n",
    "        shell.input_transformer_manager.cleanup_transforms = _transforms\n",
    "    def load_ipython_extension(shell):\n",
    "        unload_ipython_extension(shell)\n",
    "        shell.input_transformer_manager.cleanup_transforms = [cleanup_transform]\n",
    "\n",
    "    __name__ == '__main__' and load_ipython_extension(get_ipython())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
